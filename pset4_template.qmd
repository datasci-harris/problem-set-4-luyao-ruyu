---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (Ruyu Zhang and ruyuzhang):
    - Partner 2 (Luyao Guo and gluyao):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. I put six variables: PRVDR_CTGRY_SBTYP_CD (subtype of provider), PRVDR_CTGRY_CD (type of provider), FAC_NAME (name of provider), PRVDR_NUM ( CMS Certification Number), PGM_TRMNTN_CD ( current termination status), ZIP_CD(zipcode).

2. 
```{python}
import pandas as pd
import os

base_path="/Users/apple/Desktop/class/Dap-2/problem_sets/ps4/problem-set-4-luyao-ruyu"
path2016=os.path.join(base_path,"pos2016.csv")

df_2016=pd.read_csv(path2016)

import altair as alt
alt.renderers.enable("png")
import warnings 
warnings.filterwarnings('ignore')
```

    a
```{python}
hospital_2016 = df_2016[(df_2016['PRVDR_CTGRY_CD'] == 1) & (df_2016['PRVDR_CTGRY_SBTYP_CD'] == 1)]
hospital_count_2016 = hospital_2016.shape[0]
hospital_count_2016
 ```

There are 7245 short-term hospitals reported in this data.


3. 
```{python}
path2017=os.path.join(base_path,"pos2017.csv")
df_2017=pd.read_csv(path2017,encoding="ISO-8859-1")

path2018=os.path.join(base_path,"pos2018.csv")
df_2018=pd.read_csv(path2018,encoding="ISO-8859-1")

path2019=os.path.join(base_path,"pos2019.csv")
df_2019=pd.read_csv(path2019,encoding="ISO-8859-1")

hospital_2017 = df_2017[(df_2017['PRVDR_CTGRY_CD'] == 1) & (df_2017['PRVDR_CTGRY_SBTYP_CD'] == 1)]
hospital_2017['Year'] = 2017
hospital_2018 = df_2018[(df_2018['PRVDR_CTGRY_CD'] == 1) & (df_2018['PRVDR_CTGRY_SBTYP_CD'] == 1)]
hospital_2018['Year'] = 2018
hospital_2019 = df_2019[(df_2019['PRVDR_CTGRY_CD'] == 1) & (df_2019['PRVDR_CTGRY_SBTYP_CD'] == 1)]
hospital_2019['Year'] = 2019
hospital_2016['Year'] = 2016

#append 4 datasets together
hospital_all = pd.concat([hospital_2016, hospital_2017, hospital_2018, hospital_2019,], ignore_index=True)

#Plot the number of observations by years
year_all = hospital_all['Year'].value_counts().reset_index()
year_all.columns = ['Year', 'Count']

chart_all = alt.Chart(year_all).mark_bar().encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('Count:Q', title='Number of Observations',),
    tooltip=['Year', 'Count']
).properties(
    title='Number of Observations by Years',
    width=400,
    height=250
)
chart_all
```

4. 
    a.
```{python}
#plot number of unique hospitals per year
unique_hospitals = hospital_all.groupby('Year')['PRVDR_NUM'].nunique().reset_index()
unique_hospitals.columns = ['Year', 'Unique_Hospitals']

chart_unique = alt.Chart(unique_hospitals).mark_bar().encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('Unique_Hospitals:Q', title='Number of Unique Hospitals',),
    tooltip=['Year', 'Unique_Hospitals']
).properties(
    title='Number of Unique Hospitals by Years',
    width=400,
    height=250
)
chart_unique
```


    b.

## Identify hospital closures in POS file (15 pts) (*)

1. 

```{python}
df_2016 = pd.read_csv('pos2016.csv', encoding='ISO-8859-1')
df_2017 = pd.read_csv('pos2017.csv', encoding='ISO-8859-1')
df_2018 = pd.read_csv('pos2018.csv', encoding='ISO-8859-1')
df_2019 = pd.read_csv('pos2019.csv', encoding='ISO-8859-1')

# Filter active short-term hospitals in 2016
hospital_2016 = df_2016[(df_2016['PRVDR_CTGRY_CD'] == 1) & 
                        (df_2016['PRVDR_CTGRY_SBTYP_CD'] == 1) & 
                        (df_2016['PGM_TRMNTN_CD'] == 0)]
active_2016 = hospital_2016[['FAC_NAME', 'ZIP_CD', 'PRVDR_NUM']]

# Define a function to check for hospital closures
def check_closure(year_df, active_df, year):
    year_active = year_df[(year_df['PRVDR_CTGRY_CD'] == 1) & 
                          (year_df['PRVDR_CTGRY_SBTYP_CD'] == 1) & 
                          (year_df['PGM_TRMNTN_CD'] == 0)]['PRVDR_NUM']
    closed = active_df[~active_df['PRVDR_NUM'].isin(year_active)].copy()
    closed['Closure_Year'] = year
    return closed

# Check closures in subsequent years
closed_2017 = check_closure(df_2017, active_2016, 2017)
closed_2018 = check_closure(df_2018, active_2016, 2018)
closed_2019 = check_closure(df_2019, active_2016, 2019)

# Combine all closure data
all_closed = pd.concat([closed_2017, closed_2018, closed_2019]).drop_duplicates(subset='PRVDR_NUM')

# Display the number of suspected closures and the first 10 rows
num_closures = all_closed.shape[0]
print(f"Number of suspected hospital closures: {num_closures}")
print(all_closed[['FAC_NAME', 'ZIP_CD', 'Closure_Year']].head(10))
```



2. 

```{python}
sorted_closures = all_closed.sort_values(by='FAC_NAME')[['FAC_NAME', 'Closure_Year']]
print("Sorted list of the first 10 hospitals by name and their year of closure:")
print(sorted_closures.head(10))

```

3. 
    a.
```{python}
# Function to identify potential mergers/acquisitions by checking ZIP codes with stable or increasing active hospitals
def identify_mergers(df_closed, next_year_df):
    active_next_year = next_year_df[(next_year_df['PRVDR_CTGRY_CD'] == 1) & 
                                    (next_year_df['PRVDR_CTGRY_SBTYP_CD'] == 1) & 
                                    (next_year_df['PGM_TRMNTN_CD'] == 0)]
    merged_hospitals = df_closed[df_closed['ZIP_CD'].isin(active_next_year['ZIP_CD'])]
    return merged_hospitals

# Identify potential mergers for each year
potential_mergers_2017 = identify_mergers(closed_2017, df_2018)
potential_mergers_2018 = identify_mergers(closed_2018, df_2019)

# Combine all potential mergers/acquisitions and remove duplicates
potential_mergers = pd.concat([potential_mergers_2017, potential_mergers_2018]).drop_duplicates(subset='PRVDR_NUM')

# Count the number of potential mergers/acquisitions
num_potential_mergers = potential_mergers.shape[0]
print(f"Number of hospitals potentially identified as mergers/acquisitions: {num_potential_mergers}")
```
   
    b.

```{python}
# Remove potential mergers from the suspected closures to get corrected closures
corrected_closures = all_closed[~all_closed['PRVDR_NUM'].isin(potential_mergers['PRVDR_NUM'])]

# Count the number of corrected hospital closures
num_corrected_closures = corrected_closures.shape[0]
print(f"Number of corrected hospital closures: {num_corrected_closures}")

```
   
    c.
```{python}
# Sort the corrected list of closures by name and display the first 10 rows
sorted_corrected_closures = corrected_closures.sort_values(by='FAC_NAME')[['FAC_NAME', 'Closure_Year']]
print("Sorted list of the first 10 corrected hospital closures:")
print(sorted_corrected_closures.head(10))

```

## Download Census zip code shapefile (10 pt) 

1. 
    a.
    b. 
2. 
```{python}
import geopandas as gpd
import shapely
from shapely import Polygon, Point

pathshp=os.path.join(base_path,"gz_2010_us_860_00_500k.shp")
df_shp=gpd.read_file(pathshp)

# Filter Texas zip codes that start with '75', '76', '77', '78', or '79'
df_texas = df_shp[df_shp["ZCTA5"].str.startswith(('75', '76', '77', '78', '79'))]
df_texas.head()
```

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
